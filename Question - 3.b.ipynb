{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled18.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C05TRlQ-pFX2"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.sentiment import vader\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import re\n",
        "import string\n",
        "from sklearn.svm import SVR\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neural_network import MLPRegressor"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgpvsGhHr4LX",
        "outputId": "97888fbe-01e0-4315-93a6-1f2d69825b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/kdhingra307/nlp_data"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp_data'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 18 (delta 2), reused 18 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnibAlv2WmC0"
      },
      "source": [
        "class tweet:\n",
        "\n",
        "  def lexicon(self, file):\n",
        "      return [e.split(\"\\t\") for e in open(file).read().split(\"\\n\")[:-1]]\n",
        "\n",
        "  def __init__(self):\n",
        "      self.vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words='english', tokenizer=LemmaTokenizer(), preprocessor=self.preprocess)\n",
        "      self.lemmatizer = WordNetLemmatizer()\n",
        "      self.emoticon_score = {e[0]:float(e[1]) for e in self.lexicon(\"nlp_data/lexicons/emoticon.txt\")}\n",
        "      self.emotion_score = {self.preprocess(e[0]):np.array(e[1:]).astype(np.float32) for e in self.lexicon(\"nlp_data/lexicons/emotion_score.csv\")[1:]}\n",
        "      self.emotion_count = {self.preprocess(e[0, 0]):e[:, -1].astype(np.float32) for e in np.array(self.lexicon(\"nlp_data/lexicons/emotion_count.txt\")).reshape([-1, 10, 3])}\n",
        "      #self.emotion_hashtag = {self.preprocess(e[0]):float(e[1]) for e in self.lexicon(\"nlp_data/lexicons/hashtag.txt\")}\n",
        "      self.polarity_hashtag = {self.preprocess(e[0]):float(e[1]) for e in self.lexicon(\"nlp_data/lexicons/hashtag.txt\")}\n",
        "      self.polarity_sentiment = {self.preprocess(e[0]):np.array([float(e[2]), float(e[3])])  for e in self.lexicon(\"nlp_data/lexicons/polarity_sentiment.txt\")}\n",
        "      # self.polarity_sentiword = {self.preprocess(e[0]):np.array([float(e[2]), float(e[3])])  for e in self.lexicon(\"nlp_data/lexicons/polarity_sentiment.txt\")}\n",
        "      mapp = {\"positive\":0, \"negative\":1, \"neutral\":2, \"both\":3}\n",
        "      self.pwc_mpqa = {self.preprocess(e[0]):mapp[e[1]]  for e in self.lexicon(\"nlp_data/lexicons/pwc_mpqa.txt\")}\n",
        "      self.pwc_bing = {self.preprocess(e[0]):mapp[e[1]]  for e in self.lexicon(\"nlp_data/lexicons/pwc_bing_liu.csv\")}\n",
        "\n",
        "\n",
        "\n",
        "  def preprocess(self, text):\n",
        "      remove_numbers = re.sub(r'\\d+', '', text)\n",
        "      remove_punctuations = \"\".join([char.lower() for char in remove_numbers if char not in string.punctuation]) \n",
        "      remove_extra_spaces = re.sub('\\s+', ' ', remove_punctuations).strip()\n",
        "      return \" \".join([self.lemmatizer.lemmatize(t) for t in word_tokenize(remove_extra_spaces)])\n",
        "\n",
        "  def feature(self, x):\n",
        "      output = np.zeros([10+10+1+2+1+1])\n",
        "      x = preprocess(x)\n",
        "      for e in x:\n",
        "          if e in self.emotion_score:\n",
        "            output[:10] += self.emotion_score[e]\n",
        "          \n",
        "          if e in self.emotion_count:\n",
        "            output[10:20] += self.emotion_count[e]\n",
        "          \n",
        "          if e in self.polarity_hashtag:\n",
        "            output[20] += self.polarity_hashtag[e]\n",
        "          \n",
        "          if e in self.polarity_sentiment:\n",
        "            output[21:23] += self.polarity_sentiment[e]\n",
        "          \n",
        "          if e in self.pwc_mpqa:\n",
        "            output[23] += self.pwc_mpqa[e]\n",
        "          \n",
        "          if e in self.pwc_bing:\n",
        "            output[24] += self.pwc_bing[e]\n",
        "      \n",
        "      return np.concatenate([output, list(vader_analyser.polarity_scores(x).values())])"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEOuZ9l0Z_hN"
      },
      "source": [
        "tweet_parser = tweet()"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d3G8jpNuUdj",
        "outputId": "d4dcd303-d595-412a-823d-3f7cabb8404e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tweet_parser.feature(train_data[0][0])"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.56046070e+00,  2.82955130e+00,  1.83647573e+00,  1.84195930e+00,\n",
              "        1.66854285e+00,  7.75093326e+00,  7.42493064e+00,  1.15074075e+00,\n",
              "        1.25660393e+00,  5.64127346e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "       -1.86178000e+02,  2.31200000e+03,  3.81200000e+03,  0.00000000e+00,\n",
              "        0.00000000e+00,  3.17000000e-01,  6.83000000e-01,  0.00000000e+00,\n",
              "       -7.57900000e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXOjfehEwv7k",
        "outputId": "1b8a37d6-e4f7-4c54-e71f-8a4b91fa7621",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(vader_analyser.polarity_scores(train_data[0][0]).values())"
      ],
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 1.0, 0.0, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4ayEZE2o6KW"
      },
      "source": [
        "def process_data(file_name):\n",
        "  X = []\n",
        "  y = []\n",
        "  for e in open(file_name).read().split(\"\\n\")[:-1]:\n",
        "    tab_split = e.split(\"\\t\")\n",
        "    X.append(tab_split[1])\n",
        "    y.append(tab_split[3])\n",
        "  \n",
        "  return np.array(X), np.array(y).astype(np.float32)"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9GpywIsZkKK"
      },
      "source": [
        "train_data =  process_data(\"nlp_data/data/joy_train\")\n",
        "val_data =  process_data(\"nlp_data/data/joy_test\")"
      ],
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3id3n1OcfI3",
        "outputId": "a07f6dfa-355b-4188-dca5-7078f3c45aff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tweet_parser.vectorizer.fit(train_data[0])"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 2),\n",
              "                preprocessor=<bound method tweet.preprocess of <__main__.tweet object at 0x7f5a2f959978>>,\n",
              "                stop_words='english', strip_accents=None,\n",
              "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=<__main__.LemmaTokenizer object at 0x7f5a2f959438>,\n",
              "                vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 325
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4c3Z_JewVCD"
      },
      "source": [
        "def lex_feature(data):\n",
        "  return np.concatenate([tweet_parser.vectorizer.transform(data).todense(), np.array([tweet_parser.feature(e) for e in data])], axis=1)"
      ],
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIpboS05xUAQ"
      },
      "source": [
        "train_x = lex_feature(train_data[0])\n",
        "val_x = lex_feature(val_data[0])"
      ],
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0REwrv5UckCH"
      },
      "source": [
        ""
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GviuQrF1n3u7",
        "outputId": "13d40411-530d-4a8c-c64d-68650761610b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svm_regression = SVR()\n",
        "svm_regression.fit(train_x, train_data[1])\n",
        "\n",
        "print(svm_regression.fit_status_)\n",
        "\n",
        "output = svm_regression.predict(train_x)\n",
        "val_output = svm_regression.predict(val_x)\n",
        "\n",
        "print(pearsonr(val_output, val_data[1]))\n",
        "print(pearsonr(output, train_data[1]))\n",
        "\n",
        "print(spearmanr(val_output, val_data[1]))\n",
        "print(spearmanr(output, train_data[1]))"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "(0.10555881060711204, 0.004749298691680515)\n",
            "(0.15552319692330957, 7.388987894317273e-06)\n",
            "SpearmanrResult(correlation=0.10490512251543137, pvalue=0.005016365695322027)\n",
            "SpearmanrResult(correlation=0.1595378778463356, pvalue=4.238072807795342e-06)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j-f0ZkUWSnp",
        "outputId": "d1e77f57-d7c7-4452-ab15-59bd4ce4abdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        ""
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.07570299004284718, 0.03692909544883846)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGaivI9Q2jgp",
        "outputId": "53ab3eba-c5c6-4aec-9b03-4fdc2637fc44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svm_regression = DecisionTreeRegressor()\n",
        "svm_regression.fit(train_x, train_data[1])\n",
        "\n",
        "output = svm_regression.predict(train_x)\n",
        "val_output = svm_regression.predict(val_x)\n",
        "\n",
        "print(pearsonr(val_output, val_data[1]))\n",
        "print(pearsonr(output, train_data[1]))\n",
        "\n",
        "print(spearmanr(val_output, val_data[1]))\n",
        "print(spearmanr(output, train_data[1]))"
      ],
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.3272297242811444, 2.78710263423923e-19)\n",
            "(0.9998963535393343, 0.0)\n",
            "SpearmanrResult(correlation=0.31837228500844894, pvalue=2.77808686692216e-18)\n",
            "SpearmanrResult(correlation=0.9998546133040691, pvalue=0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydpi2jUr2lrk",
        "outputId": "8e05bf5a-efe8-4406-ba44-0d048cf139c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "svm_regression = MLPRegressor()\n",
        "svm_regression.fit(train_x, train_data[1])\n",
        "\n",
        "output = svm_regression.predict(train_x)\n",
        "val_output = svm_regression.predict(val_x)\n",
        "\n",
        "print(pearsonr(val_output, val_data[1]))\n",
        "print(pearsonr(output, train_data[1]))\n",
        "\n",
        "print(spearmanr(val_output, val_data[1]))\n",
        "print(spearmanr(output, train_data[1]))"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.07639710078362456, 0.04127106073465666)\n",
            "(0.2915021825375017, 1.3899664797161746e-17)\n",
            "SpearmanrResult(correlation=0.02633519253239857, pvalue=0.4823139480997083)\n",
            "SpearmanrResult(correlation=0.30736817589681953, pvalue=1.8182462269132478e-19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGbXCBdxsrxs"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rec-PfCMst0i",
        "outputId": "089f1765-ca0c-492d-bab5-cb9ba4b15ed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tSvS0w-svMa"
      },
      "source": [
        "vader_analyser."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}